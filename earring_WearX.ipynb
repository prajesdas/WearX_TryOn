{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b7d4d0a-a691-4c5a-8ab0-5e9242a033f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from tkinter import Tk, filedialog\n",
    "\n",
    "# Initialize MediaPipe modules\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1)\n",
    "\n",
    "# Load earring images (PNG with transparency)\n",
    "left_earring = cv2.imread(\"left_ear.png\", cv2.IMREAD_UNCHANGED)\n",
    "right_earring = cv2.imread(\"right_ear.png\", cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "# Ensure alpha channel exists and remove white background from earrings\n",
    "def remove_white_background(image):\n",
    "    if image is None:\n",
    "        print(\"Error: Earring image not found!\")\n",
    "        return None\n",
    "    \n",
    "    if image.shape[-1] == 3:  # Convert BGR to BGRA\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2BGRA)\n",
    "\n",
    "    # Create mask where white pixels are detected\n",
    "    gray = cv2.cvtColor(image[:, :, :3], cv2.COLOR_BGR2GRAY)\n",
    "    _, mask = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY_INV)  # White removal\n",
    "\n",
    "    # Apply mask as alpha channel\n",
    "    image[:, :, 3] = mask\n",
    "    return image\n",
    "\n",
    "left_earring = remove_white_background(left_earring)\n",
    "right_earring = remove_white_background(right_earring)\n",
    "\n",
    "# Function to overlay images with alpha blending\n",
    "def overlay_image(background, overlay, x, y, width, height):\n",
    "    if overlay is None or background is None:\n",
    "        return background\n",
    "\n",
    "    overlay = cv2.resize(overlay, (width, height))\n",
    "    alpha_mask = overlay[:, :, 3] / 255.0\n",
    "    alpha_inv = 1.0 - alpha_mask\n",
    "\n",
    "    h, w, _ = background.shape\n",
    "    x1, x2 = max(0, x), min(w, x + width)\n",
    "    y1, y2 = max(0, y), min(h, y + height)\n",
    "\n",
    "    if x1 >= x2 or y1 >= y2:\n",
    "        return background\n",
    "\n",
    "    overlay = overlay[: y2 - y1, : x2 - x1]\n",
    "    alpha_mask = alpha_mask[: y2 - y1, : x2 - x1]\n",
    "    alpha_inv = alpha_inv[: y2 - y1, : x2 - x1]\n",
    "\n",
    "    alpha_mask = np.stack([alpha_mask] * 3, axis=-1)\n",
    "    alpha_inv = np.stack([alpha_inv] * 3, axis=-1)\n",
    "\n",
    "    background[y1:y2, x1:x2, :3] = (\n",
    "        alpha_inv * background[y1:y2, x1:x2, :3] + alpha_mask * overlay[:, :, :3]\n",
    "    ).astype(np.uint8)\n",
    "\n",
    "    return background\n",
    "\n",
    "# Open file dialog to select an image\n",
    "Tk().withdraw()  # Hide the root window\n",
    "image_path = filedialog.askopenfilename(title=\"Select an Image\", filetypes=[(\"Images\", \"*.jpg;*.jpeg;*.png\")])\n",
    "\n",
    "if not image_path:\n",
    "    print(\"No image selected. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "frame = cv2.imread(image_path)\n",
    "\n",
    "if frame is None:\n",
    "    print(\"Error: Image not found!\")\n",
    "    exit()\n",
    "\n",
    "# Convert image to RGB for face detection\n",
    "frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "face_result = face_mesh.process(frame_rgb)\n",
    "\n",
    "if face_result.multi_face_landmarks:\n",
    "    face_landmarks = face_result.multi_face_landmarks[0].landmark\n",
    "\n",
    "    left_ear = face_landmarks[234]  # Landmark near left ear\n",
    "    right_ear = face_landmarks[454]  # Landmark near right ear\n",
    "    chin = face_landmarks[152]  # Chin landmark for scaling earring size\n",
    "\n",
    "    if left_ear and right_ear:\n",
    "        x1, y1 = int(left_ear.x * frame.shape[1]), int(left_ear.y * frame.shape[0])\n",
    "        x2, y2 = int(right_ear.x * frame.shape[1]), int(right_ear.y * frame.shape[0])\n",
    "        chin_y = int(chin.y * frame.shape[0])\n",
    "\n",
    "        # Earring size dynamically based on face height\n",
    "        earring_width = int(abs(x2 - x1) * 0.3)\n",
    "        earring_height = int(abs(y1 - chin_y) * 0.9)\n",
    "\n",
    "        # Move earrings downward slightly for a natural hanging effect\n",
    "        y1 += int(earring_height * 0.3)  # Move left earring down\n",
    "        y2 += int(earring_height * 0.3)  # Move right earring down\n",
    "\n",
    "        # Overlay earrings exactly below the ears\n",
    "        frame = overlay_image(frame, left_earring, x1 - earring_width // 2, y1, earring_width, earring_height)\n",
    "        frame = overlay_image(frame, right_earring, x2 - earring_width // 2, y2, earring_width, earring_height)\n",
    "\n",
    "# Resize output image to a smaller size (640px width while maintaining aspect ratio)\n",
    "output_width = 640\n",
    "scale_factor = output_width / frame.shape[1]\n",
    "output_height = int(frame.shape[0] * scale_factor)\n",
    "frame_resized = cv2.resize(frame, (output_width, output_height))\n",
    "\n",
    "# Save and display the resized final image\n",
    "output_path = \"output_with_earrings_resized.png\"\n",
    "cv2.imwrite(output_path, frame_resized)\n",
    "cv2.imshow(\"Earrings Try-On\", frame_resized)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f082c1a5-c2ec-46e6-85d9-48a4d3aecca2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
